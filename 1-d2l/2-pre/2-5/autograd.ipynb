{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f592b7e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自动微分\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如 :numref:`sec_calculus`中所说，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "498c3ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:09.704166Z",
     "iopub.status.busy": "2022-12-07T16:37:09.703482Z",
     "iopub.status.idle": "2022-12-07T16:37:10.849947Z",
     "shell.execute_reply": "2022-12-07T16:37:10.848845Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:34:12.797478Z",
     "start_time": "2025-06-18T10:34:12.472977Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ee187726",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8f4026b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.853801Z",
     "iopub.status.busy": "2022-12-07T16:37:10.853279Z",
     "iopub.status.idle": "2022-12-07T16:37:10.858081Z",
     "shell.execute_reply": "2022-12-07T16:37:10.857050Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:34:53.060844Z",
     "start_time": "2025-06-18T10:34:53.058904Z"
    }
   },
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4fd755d2",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "(**现在计算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eeb79797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.861469Z",
     "iopub.status.busy": "2022-12-07T16:37:10.861031Z",
     "iopub.status.idle": "2022-12-07T16:37:10.867917Z",
     "shell.execute_reply": "2022-12-07T16:37:10.867083Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:35:02.312309Z",
     "start_time": "2025-06-18T10:35:02.308946Z"
    }
   },
   "source": [
    "y = 2 * torch.dot(x, x) # y = 2*(x1**2 + x2**2 + x3**2 + x4**2)\n",
    "y"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "614f1281",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b21e1420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.872410Z",
     "iopub.status.busy": "2022-12-07T16:37:10.871742Z",
     "iopub.status.idle": "2022-12-07T16:37:10.877373Z",
     "shell.execute_reply": "2022-12-07T16:37:10.876674Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:35:21.489493Z",
     "start_time": "2025-06-18T10:35:21.486421Z"
    }
   },
   "source": [
    "y.backward()\n",
    "x.grad"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "e892b1d5",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f72da1f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.880808Z",
     "iopub.status.busy": "2022-12-07T16:37:10.880315Z",
     "iopub.status.idle": "2022-12-07T16:37:10.887007Z",
     "shell.execute_reply": "2022-12-07T16:37:10.886236Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:35:32.838207Z",
     "start_time": "2025-06-18T10:35:32.826029Z"
    }
   },
   "source": [
    "x.grad == 4 * x"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "355dc4bd",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "[**现在计算`x`的另一个函数。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e18e98cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.890349Z",
     "iopub.status.busy": "2022-12-07T16:37:10.889853Z",
     "iopub.status.idle": "2022-12-07T16:37:10.895860Z",
     "shell.execute_reply": "2022-12-07T16:37:10.895107Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:36:02.105317Z",
     "start_time": "2025-06-18T10:36:02.090906Z"
    }
   },
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()  # y = x1 + x2 + x3 + x4\n",
    "y.backward()\n",
    "x.grad"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "640dee7e",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "例如：\n",
    "\n",
    "y = [\n",
    "  [x1, x2, x3],\n",
    "  [x1+x2, x2, x3],\n",
    "  [x1+x2+x3, x2, x3],\n",
    "]\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "de02a306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.899324Z",
     "iopub.status.busy": "2022-12-07T16:37:10.898840Z",
     "iopub.status.idle": "2022-12-07T16:37:10.904849Z",
     "shell.execute_reply": "2022-12-07T16:37:10.904154Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T11:10:19.095439Z",
     "start_time": "2025-06-18T11:10:19.090371Z"
    }
   },
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x  # y = [x1**2, x2**2, x3**2, x4**2]\n",
    "print(x)\n",
    "print(y)\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "print(torch.ones(len(x)))\n",
    "print(y.sum())\n",
    "y.sum().backward()  # => y' = [2*x1, 2*x2, 2*x3, 2*x4]\n",
    "x.grad"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 调用sum()的原因\n",
    "\n",
    "在PyTorch中，`backward()`方法用于自动求导。当`y`是一个非标量张量时，需要通过`gradient`参数指定梯度张量，以明确如何聚合各个元素的梯度。这里调用`sum()`的原因如下：\n",
    "\n",
    "### 1. **`backward()`的工作机制**\n",
    "   - 当`y`是标量（如损失函数值）时，`y.backward()`会直接计算梯度。\n",
    "   - 当`y`是向量或张量时，PyTorch需要一个与`y`形状相同的`gradient`张量，用于指定每个元素的梯度权重。例如：\n",
    "     ```python\n",
    "     y.backward(gradient=torch.ones_like(y))  # 对每个y[i]的梯度赋予权重1\n",
    "     ```\n",
    "\n",
    "### 2. **调用`sum()`的等价性**\n",
    "   以下两种方式是等价的：\n",
    "   ```python\n",
    "   # 方式一：先求和，再对标量调用backward()\n",
    "   y.sum().backward()\n",
    "\n",
    "   # 方式二：直接对非标量调用backward()，传入全1的梯度张量\n",
    "   y.backward(gradient=torch.ones_like(y))\n",
    "   ```\n",
    "   这是因为：\n",
    "   - `y.sum()`会生成一个标量，其梯度为1。\n",
    "   - 根据链式法则，每个`x.grad`的元素会等于`dy/dx * 1`，即`2*x`。\n",
    "\n",
    "### 3. **为什么需要这样做？**\n",
    "   在深度学习中，我们通常需要计算损失函数（标量）对参数的梯度。但在某些场景（如计算雅克比矩阵）中，输出`y`可能是向量。此时：\n",
    "   - **如果直接`y.backward()`**，PyTorch会报错，因为它不知道如何聚合多个输出的梯度。\n",
    "   - **传入`gradient=torch.ones_like(y)`** 相当于对每个输出元素的梯度赋予相同权重（即求和），得到的结果与先`sum()`再求导一致。\n",
    "\n",
    "### 4. **示例验证**\n",
    "   ```python\n",
    "   x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "   # 方式一：先求和\n",
    "   x.grad.zero_()\n",
    "   y = x * x\n",
    "   y.sum().backward()\n",
    "   print(x.grad)  # 输出：tensor([2., 4., 6., 8.])\n",
    "\n",
    "   # 方式二：直接传入梯度\n",
    "   x.grad.zero_()\n",
    "   y = x * x\n",
    "   y.backward(gradient=torch.ones_like(y))\n",
    "   print(x.grad)  # 输出：tensor([2., 4., 6., 8.])\n",
    "   ```\n",
    "\n",
    "### 总结\n",
    "这里调用`sum()`的目的是将向量`y`转换为标量，从而避免显式传递`gradient`参数。两种方式在数学上等价，但`sum().backward()`更简洁。本质上，这是PyTorch处理非标量输出梯度的机制。"
   ],
   "id": "d9c8864d08ebacf0"
  },
  {
   "cell_type": "markdown",
   "id": "ca55ef50",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0d0a164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.908385Z",
     "iopub.status.busy": "2022-12-07T16:37:10.907775Z",
     "iopub.status.idle": "2022-12-07T16:37:10.915808Z",
     "shell.execute_reply": "2022-12-07T16:37:10.914379Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:38:25.728187Z",
     "start_time": "2025-06-18T10:38:25.714804Z"
    }
   },
   "source": [
    "x.grad.zero_()\n",
    "y = x * x  # y = x**2\n",
    "u = y.detach()\n",
    "z = u * x  # # z = y(detached) * x\n",
    "\n",
    "z.sum().backward()  # z' = y(detached)\n",
    "x.grad == u"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "e7d753da",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "16899140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.920202Z",
     "iopub.status.busy": "2022-12-07T16:37:10.919752Z",
     "iopub.status.idle": "2022-12-07T16:37:10.927012Z",
     "shell.execute_reply": "2022-12-07T16:37:10.925890Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:38:36.551553Z",
     "start_time": "2025-06-18T10:38:36.546687Z"
    }
   },
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()  # y' = 2*x\n",
    "x.grad == 2 * x"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "c8831f3d",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "<font color=\"#f00\">[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。</font>\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4eb430b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.930293Z",
     "iopub.status.busy": "2022-12-07T16:37:10.929875Z",
     "iopub.status.idle": "2022-12-07T16:37:10.935285Z",
     "shell.execute_reply": "2022-12-07T16:37:10.934140Z"
    },
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:39:14.098196Z",
     "start_time": "2025-06-18T10:39:14.096061Z"
    }
   },
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    print(f\"b: {b}\")\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "函数 f(a) 的逻辑：\n",
    "\n",
    "- 初始放大：将输入张量 a 乘以 2，得到 b。\n",
    "- 循环放大：只要 b 的范数（L2 范数）小于 1000，就将 b 继续乘以 2。\n",
    "- 条件选择：\n",
    "    - 如果 b 的元素总和为正，令 c = b。\n",
    "    - 否则，令 c = 100 * b。\n",
    "    - 返回结果：返回 c。\n"
   ],
   "id": "3cddf6d1003655a5"
  },
  {
   "cell_type": "markdown",
   "id": "b7ae43cf",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "让我们计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4251a67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.938498Z",
     "iopub.status.busy": "2022-12-07T16:37:10.937961Z",
     "iopub.status.idle": "2022-12-07T16:37:10.946161Z",
     "shell.execute_reply": "2022-12-07T16:37:10.945110Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:39:21.125597Z",
     "start_time": "2025-06-18T10:39:21.114543Z"
    }
   },
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "6b607640",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`，因此可以用`d/a`验证梯度是否正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd3e9d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:10.949650Z",
     "iopub.status.busy": "2022-12-07T16:37:10.949215Z",
     "iopub.status.idle": "2022-12-07T16:37:10.956034Z",
     "shell.execute_reply": "2022-12-07T16:37:10.954996Z"
    },
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-18T10:39:31.385395Z",
     "start_time": "2025-06-18T10:39:31.373607Z"
    }
   },
   "source": [
    "a.grad == d / a"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "74238334",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "练习1:\n",
    "\n",
    "计算二阶导数是在一阶导数的基础上进行的，自然开销要大：\n",
    "\n",
    "https://baike.baidu.com/item/%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0#:~:text=%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E6%98%AF%E4%B8%80,%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E5%87%B9%E5%87%B8%E6%80%A7%E3%80%82"
   ],
   "id": "db1b31a34d09fabf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:36:40.366597Z",
     "start_time": "2025-06-18T11:36:40.360047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习2:\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.arange(10.0, requires_grad=True)\n",
    "y = 2 * x.dot(x) # => y = 2*(x1**2 + x2**2 + x3**2 + x4**2 + ... + x10**2)\n",
    "# print(x)\n",
    "# print(y)\n",
    "y.backward() # y' = [4*x1, 4*x2, 4*x3, ..., 4*x10]\n",
    "print(x.grad)\n",
    "# y.backward()\n",
    "# RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
   ],
   "id": "8c01213e15b39525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:36:31.626030Z",
     "start_time": "2025-06-18T11:36:31.622439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习2: fix\n",
    "import torch\n",
    "\n",
    "x = torch.arange(10.0, requires_grad=True)\n",
    "y = 2 * x.dot(x) # => y = 2*(x1**2 + x2**2 + x3**2 + x4**2 + ... + x10**2)\n",
    "y.backward(retain_graph=True) # y' = [4*x1, 4*x2, 4*x3, ..., 4*x10]\n",
    "print(x.grad)\n",
    "y.backward()\n",
    "print(x.grad) # Calculate the result!\n",
    "# If we use y.backward(retain_graph=True) then we can run y.backward() again as it will do one more time the computation graph"
   ],
   "id": "4a6e187d363d7b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.])\n",
      "tensor([ 0.,  8., 16., 24., 32., 40., 48., 56., 64., 72.])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:47:39.639861Z",
     "start_time": "2025-06-18T11:47:39.634072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习3:\n",
    "\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    print(f\"b: {b}\")\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(3,3), requires_grad=True)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "d = f(a)\n",
    "print(f\"d: {d}\") # 2**n * (100 or 1) * a (n == 9)\n",
    "# d.backward() # <====== run time error if a is vector or matrix RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "d.sum().backward() # <===== this way it will work\n",
    "print(a.grad) # result: 2**n * (100 or 1)"
   ],
   "id": "1d3c90e15c8b93d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[ 0.2973,  0.7093,  0.7847],\n",
      "        [ 0.0258,  0.7414, -1.8574],\n",
      "        [ 1.0937, -0.1498,  1.1194]], requires_grad=True)\n",
      "b: tensor([[ 152.2222,  363.1816,  401.7459],\n",
      "        [  13.2009,  379.5918, -950.9648],\n",
      "        [ 559.9799,  -76.6838,  573.1300]], grad_fn=<MulBackward0>)\n",
      "d: tensor([[ 152.2222,  363.1816,  401.7459],\n",
      "        [  13.2009,  379.5918, -950.9648],\n",
      "        [ 559.9799,  -76.6838,  573.1300]], grad_fn=<MulBackward0>)\n",
      "tensor([[512., 512., 512.],\n",
      "        [512., 512., 512.],\n",
      "        [512., 512., 512.]])\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:56:14.384818Z",
     "start_time": "2025-06-18T11:56:14.380240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习4:\n",
    "\n",
    "def f2(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    print(f\"b: {b}\")\n",
    "\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = b * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(3,3), requires_grad=True)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "d = f2(a)\n",
    "print(f\"f2(a): {d}\") # 2**n * (100 or 1) * a (n == 9)\n",
    "# d.backward() # <====== run time error if a is vector or matrix RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "d.sum().backward() # <===== this way it will work\n",
    "print(a.grad) # result: 2**n * (100 or 1)"
   ],
   "id": "e1a88a152af09b32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[ 0.7074, -2.6356,  0.8060],\n",
      "        [-0.6334, -1.7410,  0.9167],\n",
      "        [-1.1983,  1.8249,  0.8040]], requires_grad=True)\n",
      "b: tensor([[ 181.0841, -674.7144,  206.3418],\n",
      "        [-162.1581, -445.6984,  234.6843],\n",
      "        [-306.7547,  467.1632,  205.8339]], grad_fn=<MulBackward0>)\n",
      "f2(a): tensor([[ 32791.4531, 455239.5312,  42576.9297],\n",
      "        [ 26295.2383, 198647.0312,  55076.7031],\n",
      "        [ 94098.4453, 218241.4375,  42367.5898]], grad_fn=<MulBackward0>)\n",
      "tensor([[  92715.0625, -345453.7812,  105646.9922],\n",
      "        [ -83024.9297, -228197.5625,  120158.3438],\n",
      "        [-157058.4062,  239187.5469,  105386.9531]])\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T12:08:48.941413Z",
     "start_time": "2025-06-18T12:08:48.842572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习5:\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def use_svg_display():  #@save\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):  #@save\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"绘制数据点\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "\n",
    "    # 如果X有一个轴，输出True\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            # 确保x和y是分离的张量\n",
    "            x = x.detach().numpy() if isinstance(x, torch.Tensor) else x\n",
    "            y = y.detach().numpy() if isinstance(y, torch.Tensor) else y\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "x = torch.arange(-2*torch.pi, torch.pi*2, 0.1,requires_grad=True)\n",
    "y = torch.sin(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "y.sum().backward()\n",
    "y1 = x.grad\n",
    "\n",
    "plot(x, [y, y1], 'x', 'sin(x)', legend=['f(x)', \"f'(x)\"])"
   ],
   "id": "df4ca747f6a02cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([126])\n",
      "torch.Size([126])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"254.660937pt\" height=\"183.35625pt\" viewBox=\"0 0 254.660937 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-06-18T20:08:48.924881</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 254.660937 183.35625 \nL 254.660937 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.160938 145.8 \nL 247.460938 145.8 \nL 247.460938 7.2 \nL 52.160938 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 79.26411 145.8 \nL 79.26411 7.2 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m738ea48f55\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m738ea48f55\" x=\"79.26411\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −5.0 -->\n      <g transform=\"translate(67.122704 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 114.773201 145.8 \nL 114.773201 7.2 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m738ea48f55\" x=\"114.773201\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2.5 -->\n      <g transform=\"translate(102.631795 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 150.282292 145.8 \nL 150.282292 7.2 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m738ea48f55\" x=\"150.282292\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.0 -->\n      <g transform=\"translate(142.33073 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 185.791383 145.8 \nL 185.791383 7.2 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m738ea48f55\" x=\"185.791383\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2.5 -->\n      <g transform=\"translate(177.83982 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 221.300474 145.8 \nL 221.300474 7.2 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m738ea48f55\" x=\"221.300474\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 5.0 -->\n      <g transform=\"translate(213.348911 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- x -->\n     <g transform=\"translate(146.851563 174.076563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 52.160938 139.500616 \nL 247.460938 139.500616 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"mbde4f943f2\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mbde4f943f2\" x=\"52.160938\" y=\"139.500616\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −1.0 -->\n      <g transform=\"translate(20.878125 143.299835)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 52.160938 108.000462 \nL 247.460938 108.000462 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mbde4f943f2\" x=\"52.160938\" y=\"108.000462\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.5 -->\n      <g transform=\"translate(20.878125 111.799681)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 52.160938 76.500308 \nL 247.460938 76.500308 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mbde4f943f2\" x=\"52.160938\" y=\"76.500308\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(29.257812 80.299527)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 52.160938 45.000154 \nL 247.460938 45.000154 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mbde4f943f2\" x=\"52.160938\" y=\"45.000154\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <g transform=\"translate(29.257812 48.799373)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 52.160938 13.5 \nL 247.460938 13.5 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#mbde4f943f2\" x=\"52.160938\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(29.257812 17.299219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- sin(x) -->\n     <g transform=\"translate(14.798438 90.523438)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"52.099609\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"79.882812\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"143.261719\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"182.275391\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"241.455078\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 61.03821 76.500319 \nL 62.458573 70.210789 \nL 63.878935 63.984102 \nL 65.299304 57.882443 \nL 66.719666 51.966837 \nL 68.140028 46.29636 \nL 69.560391 40.927672 \nL 70.980753 35.914411 \nL 72.401122 31.306654 \nL 73.821484 27.150474 \nL 75.241847 23.487382 \nL 76.662216 20.353964 \nL 78.082578 17.781555 \nL 79.50294 15.795845 \nL 80.923302 14.416671 \nL 82.343665 13.657816 \nL 83.764034 13.526864 \nL 85.184396 14.025122 \nL 86.604758 15.147608 \nL 88.025127 16.883118 \nL 89.44549 19.214299 \nL 90.865852 22.11786 \nL 92.286214 25.564789 \nL 93.70658 29.520659 \nL 95.126939 33.945915 \nL 96.547301 38.796369 \nL 97.967667 44.023562 \nL 99.388029 49.575237 \nL 100.808395 55.395952 \nL 102.228757 61.427522 \nL 103.649119 67.609693 \nL 105.069485 73.880711 \nL 106.489851 80.177904 \nL 107.910213 86.438336 \nL 109.330579 92.599486 \nL 110.750941 98.599763 \nL 112.171307 104.379243 \nL 113.591669 109.880154 \nL 115.012031 115.047543 \nL 116.432397 119.829794 \nL 117.852759 124.179102 \nL 119.273121 128.052015 \nL 120.693487 131.409854 \nL 122.113849 134.219045 \nL 123.534213 136.451533 \nL 124.954577 138.085007 \nL 126.374941 139.103149 \nL 127.795305 139.495783 \nL 129.215667 139.258986 \nL 130.636031 138.395126 \nL 132.056395 136.91283 \nL 133.476758 134.826918 \nL 134.897122 132.158225 \nL 136.317486 128.933411 \nL 137.73785 125.184706 \nL 139.158213 120.949564 \nL 140.578577 116.270298 \nL 141.998941 111.193666 \nL 143.419304 105.770388 \nL 144.839668 100.054651 \nL 146.260031 94.103568 \nL 147.680395 87.9766 \nL 149.100759 81.734964 \nL 150.521122 75.441025 \nL 151.941485 69.157672 \nL 153.361849 62.947682 \nL 154.782213 56.873107 \nL 156.202576 50.994639 \nL 157.62294 45.371014 \nL 159.043303 40.058424 \nL 160.463667 35.10995 \nL 161.884031 30.575034 \nL 163.304394 26.498989 \nL 164.724758 22.922538 \nL 166.145122 19.881421 \nL 167.565485 17.406021 \nL 168.985849 15.521072 \nL 170.406213 14.245405 \nL 171.826577 13.591771 \nL 173.246941 13.566695 \nL 174.667303 14.170434 \nL 176.087667 15.396951 \nL 177.508031 17.233992 \nL 178.928395 19.663204 \nL 180.348757 22.660312 \nL 181.769119 26.195369 \nL 183.189485 30.233064 \nL 184.609847 34.733035 \nL 186.030213 39.650345 \nL 187.450575 44.935836 \nL 188.870941 50.53672 \nL 190.291303 56.397012 \nL 191.711669 62.458181 \nL 193.132031 68.659642 \nL 194.552393 74.939443 \nL 195.972759 81.234856 \nL 197.393121 87.482947 \nL 198.813484 93.621303 \nL 200.233849 99.588608 \nL 201.654212 105.325206 \nL 203.074577 110.77381 \nL 204.49494 115.87995 \nL 205.915302 120.592622 \nL 207.335667 124.864752 \nL 208.75603 128.653629 \nL 210.176392 131.921405 \nL 211.596754 134.635438 \nL 213.017123 136.768611 \nL 214.437486 138.299592 \nL 215.857848 139.213098 \nL 217.27821 139.5 \nL 218.698573 139.157429 \nL 220.118942 138.188805 \nL 221.539304 136.603814 \nL 222.959666 134.418291 \nL 224.380028 131.654071 \nL 225.800397 128.338755 \nL 227.22076 124.505504 \nL 228.641122 120.192601 \nL 230.061484 115.443141 \nL 231.481847 110.304575 \nL 232.902216 104.828225 \nL 234.322578 99.068857 \nL 235.74294 93.083991 \nL 237.163302 86.933427 \nL 238.583665 80.678619 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 61.03821 13.5 \nL 62.458573 13.814738 \nL 63.878935 14.755807 \nL 65.299304 16.313814 \nL 66.719666 18.473179 \nL 68.140028 21.21233 \nL 69.560391 24.503902 \nL 70.980753 28.315002 \nL 72.401122 32.607571 \nL 73.821484 37.338684 \nL 75.241847 42.461087 \nL 76.662216 47.923624 \nL 78.082578 53.671665 \nL 79.50294 59.6478 \nL 80.923302 65.792321 \nL 82.343665 72.043831 \nL 83.764034 78.3399 \nL 85.184396 84.617558 \nL 86.604758 90.814111 \nL 88.025127 96.867673 \nL 89.44549 102.717704 \nL 90.865852 108.305779 \nL 92.286214 113.576066 \nL 93.70658 118.475915 \nL 95.126939 122.956337 \nL 96.547301 126.972597 \nL 97.967667 130.484561 \nL 99.388029 133.457129 \nL 100.808395 135.860607 \nL 102.228757 137.670972 \nL 103.649119 138.87014 \nL 105.069485 139.446129 \nL 106.489851 139.393186 \nL 107.910213 138.711836 \nL 109.330579 137.408891 \nL 110.750941 135.497367 \nL 112.171307 132.996358 \nL 113.591669 129.930866 \nL 115.012031 126.331517 \nL 116.432397 122.234259 \nL 117.852759 117.680053 \nL 119.273121 112.714395 \nL 120.693487 107.386884 \nL 122.113849 101.750778 \nL 123.534213 95.862372 \nL 124.954577 89.780505 \nL 126.374941 83.565946 \nL 127.795305 77.280791 \nL 129.215667 70.987844 \nL 130.636031 64.749969 \nL 132.056395 58.629498 \nL 133.476758 52.687595 \nL 134.897122 46.983613 \nL 136.317486 41.574554 \nL 137.73785 36.514459 \nL 139.158213 31.853893 \nL 140.578577 27.63942 \nL 141.998941 23.913144 \nL 143.419304 20.712306 \nL 144.839668 18.068881 \nL 146.260031 16.009282 \nL 147.680395 14.554093 \nL 149.100759 13.717849 \nL 150.521122 13.508907 \nL 151.941485 13.929352 \nL 153.361849 14.974985 \nL 154.782213 16.63536 \nL 156.202576 18.893886 \nL 157.62294 21.728 \nL 159.043303 25.109376 \nL 160.463667 29.004234 \nL 161.884031 33.373658 \nL 163.304394 38.173986 \nL 164.724758 43.357263 \nL 166.145122 48.871694 \nL 167.565485 54.662173 \nL 168.985849 60.670859 \nL 170.406213 66.837707 \nL 171.826577 73.101101 \nL 173.246941 79.398458 \nL 174.667303 85.666851 \nL 176.087667 91.843662 \nL 177.508031 97.867169 \nL 178.928395 103.677184 \nL 180.348757 109.21565 \nL 181.769119 114.427233 \nL 183.189485 119.259877 \nL 184.609847 123.665272 \nL 186.030213 127.599419 \nL 187.450575 131.022991 \nL 188.870941 133.901798 \nL 190.291303 136.207061 \nL 191.711669 137.915756 \nL 193.132031 139.01081 \nL 194.552393 139.481277 \nL 195.972759 139.322458 \nL 197.393121 138.535947 \nL 198.813484 137.129594 \nL 200.233849 135.117451 \nL 201.654212 132.519628 \nL 203.074577 129.362072 \nL 204.49494 125.676344 \nL 205.915302 121.499271 \nL 207.335667 116.872569 \nL 208.75603 111.842492 \nL 210.176392 106.459291 \nL 211.596754 100.776749 \nL 213.017123 94.851616 \nL 214.437486 88.743151 \nL 215.857848 82.51236 \nL 217.27821 76.221499 \nL 218.698573 69.933423 \nL 220.118942 63.710932 \nL 221.539304 57.616259 \nL 222.959666 51.710267 \nL 224.380028 46.051968 \nL 225.800397 40.697874 \nL 227.22076 35.701534 \nL 228.641122 31.112841 \nL 230.061484 26.977642 \nL 231.481847 23.337257 \nL 232.902216 20.228044 \nL 234.322578 17.681098 \nL 235.74294 15.721854 \nL 237.163302 14.36989 \nL 238.583665 13.63871 \n\" clip-path=\"url(#pac54d19a02)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 52.160938 145.8 \nL 52.160938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 247.460938 145.8 \nL 247.460938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 52.160938 145.8 \nL 247.460938 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 52.160938 7.2 \nL 247.460938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 188.470313 44.55625 \nL 240.460938 44.55625 \nQ 242.460938 44.55625 242.460938 42.55625 \nL 242.460938 14.2 \nQ 242.460938 12.2 240.460938 12.2 \nL 188.470313 12.2 \nQ 186.470313 12.2 186.470313 14.2 \nL 186.470313 42.55625 \nQ 186.470313 44.55625 188.470313 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 190.470313 20.298438 \nL 200.470313 20.298438 \nL 210.470313 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- f(x) -->\n     <g transform=\"translate(218.470313 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 190.470313 34.976562 \nL 200.470313 34.976562 \nL 210.470313 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- f'(x) -->\n     <g transform=\"translate(218.470313 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-27\" d=\"M 1147 4666 \nL 1147 2931 \nL 616 2931 \nL 616 4666 \nL 1147 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-27\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"62.695312\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"101.708984\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"160.888672\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pac54d19a02\">\n   <rect x=\"52.160938\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 报错：RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "\n",
    "这个错误是由于尝试在需要梯度计算的张量上调用 numpy() 方法导致的。在 PyTorch 中，若张量设置了 requires_grad=True，则无法直接转换为 NumPy 数组，需要先使用 detach() 方法分离梯度计算图。\n",
    "\n",
    "错误原因：\n",
    "在 plot() 函数中，可能存在类似 y.numpy() 的调用，而此时 y 是设置了 requires_grad=True 的张量。Matplotlib 的 plot() 函数需要 NumPy 数组作为输入，因此报错。\n",
    "\n",
    "解决方案：\n",
    "修改 plot() 函数，在调用 numpy() 之前先对张量使用 detach() 方法！\n",
    "\n",
    "解释：\n",
    "detach () 方法：创建一个新的张量，与原张量共享数据但不参与梯度计算。\n",
    "修改后的 plot () 函数：在将张量转换为 NumPy 数组前，先检查是否为 PyTorch 张量并调用 detach()，确保不会触发梯度相关错误。"
   ],
   "id": "d848f3ab44cb0cd2"
  },
  {
   "cell_type": "markdown",
   "id": "ba845e3d",
   "metadata": {
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1759)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
